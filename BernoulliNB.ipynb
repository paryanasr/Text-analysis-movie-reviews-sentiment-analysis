{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BernoulliNB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd7VxDzHYwzn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "7037a548-acfb-4086-c8b3-9d49d058f41b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtmkWMjHY2Tl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "outputId": "985e8dc3-bbf7-4747-f36a-248bb0bc5f08"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import sys\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "maxFeatures = 5000  # Maximum number of words to use with the vectorizer\n",
        "inColab = False  # No need to change this, it's checked automatically\n",
        "\n",
        "\n",
        "# Split data up, return a dictionary where keys are class names and values are lists of entries\n",
        "def separateClasses(data, labelVector):\n",
        "    copyVector = labelVector.to_numpy()\n",
        "    tempDict = dict()\n",
        "    tempDict['positive'] = list()\n",
        "    tempDict['negative'] = list()\n",
        "    for x in range(len(data)):\n",
        "        if copyVector[x] == 'positive':\n",
        "            tempDict['positive'].append(data[x])\n",
        "        else:\n",
        "            tempDict['negative'].append(data[x])\n",
        "    tempDict['positive'] = np.array(tempDict['positive'], dtype=np.uint32)\n",
        "    tempDict['negative'] = np.array(tempDict['negative'], dtype=np.uint32)\n",
        "    return tempDict\n",
        "\n",
        "\n",
        "# Train the algorithm. Produces the vectorizer and thetas used for classification\n",
        "def train(trainingData, stopWords):\n",
        "    # Create the vectorizer and fit it to the data. Transform the training data to word counts.\n",
        "    vectorizer = CountVectorizer(strip_accents='ascii', lowercase=True, analyzer='word', token_pattern='(?u)[a-zA-Z]+',\n",
        "                                 stop_words=stopWords, max_features=maxFeatures, binary=True)\n",
        "    countMatrix = vectorizer.fit_transform(trainingData['review'])\n",
        "    countMatrixAsArray = countMatrix.toarray()  # This line gets us the training examples as a matrix, where rows are reviews and columns are word counts\n",
        "    numWords = len(countMatrixAsArray[0])\n",
        "\n",
        "    # ----- Uncomment the following lines to see some info printed out during the training process -----\n",
        "    # print(\"Show the\", maxFeatures, \"most popular words in the corpus:\\n\", vectorizer.get_feature_names())\n",
        "    # print(\"\\nNumber of reviews (rows) in the word count table:\", len(countMatrixAsArray))\n",
        "    # print(\"Number of words   (cols) in the word count table:\", len(countMatrixAsArray[0]))\n",
        "\n",
        "    # Create dictionary with separated classes\n",
        "    separated = separateClasses(countMatrixAsArray, trainingData['sentiment'])\n",
        "\n",
        "    # Calculate theta0 and theta1\n",
        "    totalNegReviews = len(separated['negative'])\n",
        "    totalPosReviews = len(separated['positive'])\n",
        "    totalReviews = totalNegReviews + totalPosReviews\n",
        "    theta0 = totalNegReviews / totalReviews\n",
        "    theta1 = totalPosReviews / totalReviews\n",
        "\n",
        "    # Calculate theta_j_0 and theta_j_1\n",
        "    negCounts = separated['negative'].sum(axis=0)\n",
        "    posCounts = separated['positive'].sum(axis=0)\n",
        "    theta_j_0 = np.array([0 for _ in range(numWords)], dtype='float64')\n",
        "    theta_j_1 = np.array([0 for _ in range(numWords)], dtype='float64')\n",
        "    for x in range(numWords):\n",
        "        theta_j_0[x] = (negCounts[x] + 1) / (totalNegReviews + numWords)\n",
        "        theta_j_1[x] = (posCounts[x] + 1) / (totalPosReviews + numWords)\n",
        "\n",
        "    # Return all we'll need for testing\n",
        "    return numWords, vectorizer, theta0, theta1, theta_j_0, theta_j_1\n",
        "\n",
        "\n",
        "# Test how well our algorithm can classify new examples\n",
        "def test(vectorizer, testData, theta0, theta1, theta_j_0, theta_j_1):\n",
        "    # Convert the reviews from words to to word counts in accordance with the vectorizer we used in training\n",
        "    transformedData = vectorizer.transform(testData['review'])\n",
        "    dataToArray = transformedData.toarray()\n",
        "\n",
        "    # For convenience and clarity, assign the sentiment column to \"labelVector\"\n",
        "    labelVector = testData['sentiment'].to_numpy()\n",
        "\n",
        "    # This complicated looking mess is simply a log-likelihood calculation that's been compressed to be memory efficient\n",
        "    # It was originally done using multiple variables but this proved to be problematic because of very high memory usage\n",
        "    fastLogLikelihoods = np.add(np.add(np.dot(dataToArray, np.log(np.true_divide(theta_j_1, theta_j_0))), np.dot(\n",
        "        np.subtract(np.ones((len(dataToArray), len(dataToArray[0]))), dataToArray), np.log(\n",
        "            np.true_divide(np.subtract(np.ones(len(theta_j_1)), theta_j_1),\n",
        "                           np.subtract(np.ones(len(theta_j_0)), theta_j_0))))), np.log(theta1 / theta0))\n",
        "\n",
        "    # Loop over our prediction vector and count up how many correct predictions were made\n",
        "    correctPredictions = 0\n",
        "    for i in range(len(testData)):\n",
        "        if fastLogLikelihoods[i] > 0 and labelVector[i] == 'positive':\n",
        "            correctPredictions += 1\n",
        "        if fastLogLikelihoods[i] < 0 and labelVector[i] == 'negative':\n",
        "            correctPredictions += 1\n",
        "\n",
        "    # Return an accuracy score out of 100\n",
        "    return 100 * correctPredictions / len(labelVector)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # To allow for easier cross platform execution, check if this file is being run in a Google Colab notebook\n",
        "    inColab = 'google.colab' in sys.modules\n",
        "\n",
        "    # If we're using Colab, then we can work with bigger amounts of data\n",
        "    if inColab:\n",
        "        print(\"Colab environment detected.\\n\")\n",
        "        trainingData = pd.read_csv(\"./gdrive/My Drive/train.csv\")  # | review (text)  | sentiment (pos/neg) |\n",
        "        # testData = pd.read_csv(\"test.csv\")       # | id (review id) |   review (text)     |\n",
        "        stopWords = [line.rstrip('\\n') for line in open(\"./gdrive/My Drive/stopwords.txt\")]\n",
        "\n",
        "        # Take n random samples, where n = (number of rows in dataset) * frac\n",
        "        # Setting frac=1 is the same as shuffling the rows of the dataset\n",
        "        trainingData = trainingData.sample(frac=1)\n",
        "\n",
        "        # Do K-Fold cross-validation. Call our train/test functions\n",
        "        # Print the accuracy of each fold and the average accuracy across all folds\n",
        "        kf = KFold(n_splits=5, shuffle=False)\n",
        "        foldNum = 0\n",
        "        foldAccs = []\n",
        "        for train_index, test_index in kf.split(trainingData):\n",
        "            print(\"Fold\", foldNum + 1)\n",
        "\n",
        "            print(\"Training...\", end='')\n",
        "            kfoldTrain = trainingData.iloc[train_index]\n",
        "            numWords, vectorizer, theta0, theta1, theta_j_0, theta_j_1 = train(kfoldTrain, stopWords)\n",
        "            print(\"DONE\")\n",
        "\n",
        "            print(\"Testing...\", end='')\n",
        "            kfoldTest = trainingData.iloc[test_index]\n",
        "            acc = test(vectorizer, kfoldTest, theta0, theta1, theta_j_0, theta_j_1)\n",
        "            print(\"DONE \\nAccuracy = {0:.2f}\".format(acc), \"%\\n\")\n",
        "\n",
        "            foldAccs.append(acc)\n",
        "            foldNum += 1\n",
        "        print(\"Average accuracy across all folds = {0:.2f}%\".format(np.array(foldAccs).sum() / len(foldAccs)))\n",
        "\n",
        "\n",
        "    # If we're not using Colab, then we'll work with more modest amounts of data (between 1/4 to 1/2 or so)\n",
        "    else:\n",
        "        print(\"Colab environment not detected.\\n\")\n",
        "        trainingData = pd.read_csv(\"train.csv\")  # | review (text)  | sentiment (pos/neg) |\n",
        "        # testData = pd.read_csv(\"test.csv\")       # | id (review id) |   review (text)     |\n",
        "        stopWords = [line.rstrip('\\n') for line in open(\"stopwords.txt\")]\n",
        "\n",
        "        # Take n random samples, where n = (number of rows in dataset) * frac\n",
        "        # Setting frac=0.5 means we're randomly picking half of the rows in the dataset\n",
        "        trainingData = trainingData.sample(frac=0.5)\n",
        "\n",
        "        # Do K-Fold cross-validation. Call our train/test functions\n",
        "        # Print the accuracy of each fold and the average accuracy across all folds\n",
        "        kf = KFold(n_splits=10, shuffle=False)\n",
        "        foldNum = 0\n",
        "        foldAccs = []\n",
        "        for train_index, test_index in kf.split(trainingData):\n",
        "            print(\"Fold\", foldNum + 1)\n",
        "\n",
        "            print(\"Training...\", end='')\n",
        "            kfoldTrain = trainingData.iloc[train_index]\n",
        "            numWords, vectorizer, theta0, theta1, theta_j_0, theta_j_1 = train(kfoldTrain, stopWords)\n",
        "            print(\"DONE\")\n",
        "\n",
        "            print(\"Testing...\", end='')\n",
        "            kfoldTest = trainingData.iloc[test_index]\n",
        "            acc = test(vectorizer, kfoldTest, theta0, theta1, theta_j_0, theta_j_1)\n",
        "            print(\"DONE \\nAccuracy = {0:.2f}\".format(acc), \"%\\n\")\n",
        "\n",
        "            foldAccs.append(acc)\n",
        "            foldNum += 1\n",
        "        print(\"Average accuracy across all folds = {0:.2f}%\".format(np.array(foldAccs).sum() / len(foldAccs)))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Colab environment detected.\n",
            "\n",
            "Fold 1\n",
            "Training...DONE\n",
            "Testing...DONE \n",
            "Accuracy = 83.68 %\n",
            "\n",
            "Fold 2\n",
            "Training...DONE\n",
            "Testing...DONE \n",
            "Accuracy = 84.30 %\n",
            "\n",
            "Fold 3\n",
            "Training...DONE\n",
            "Testing...DONE \n",
            "Accuracy = 84.02 %\n",
            "\n",
            "Fold 4\n",
            "Training...DONE\n",
            "Testing...DONE \n",
            "Accuracy = 82.78 %\n",
            "\n",
            "Fold 5\n",
            "Training...DONE\n",
            "Testing...DONE \n",
            "Accuracy = 83.93 %\n",
            "\n",
            "Average accuracy across all folds = 83.74%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}