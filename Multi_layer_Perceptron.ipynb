{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-layer Perceptron.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "59iiZOy-gTMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsIALHCtcMk8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "a84a14a0-09fa-472e-976f-3bb1e4c9c2cb"
      },
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import time\n",
        "\n",
        "# Set this variable to True to train on entire dataset. This means we have no test set to check accuracy with. No accuracy output\n",
        "# Set it to False to do a train_test_split. This means we have a test set that we can check accuracy with.\n",
        "trainOnEntireDataSet = False\n",
        "\n",
        "# Stemmer function to reduce feature space\n",
        "def stemmed_words(doc):\n",
        "    return (stemmer.stem(w) for w in analyzer(doc))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # To allow for easier cross platform execution, check if this file is being run in a Google Colab notebook\n",
        "    inColab = 'google.colab' in sys.modules\n",
        "\n",
        "    #Import datasets\n",
        "    if inColab:\n",
        "        trainingData = pd.read_csv(\"./gdrive/My Drive/train.csv\")\n",
        "        stopWords = [line.rstrip('\\n') for line in open(\"./gdrive/My Drive/stopwords.txt\")]\n",
        "        testData = pd.read_csv(\"./gdrive/My Drive/test.csv\")\n",
        "\n",
        "    else:\n",
        "        trainingData = pd.read_csv(\"train.csv\")\n",
        "        stopWords = [line.rstrip('\\n') for line in open(\"stopwords.txt\")]\n",
        "        testData = pd.read_csv(\"test.csv\")\n",
        "\n",
        "    startTime = time.time()\n",
        "\n",
        "    # Adjust data sets if we're producing a file to submit to Kaggle\n",
        "    if trainOnEntireDataSet:\n",
        "        trainingData = trainingData.sample(frac=1)\n",
        "        X_train = trainingData['review']\n",
        "        y_train = trainingData['sentiment']\n",
        "        X_test = testData['review']\n",
        "        y_test = None\n",
        "\n",
        "    else:\n",
        "        trainingData = trainingData.sample(frac=0.1)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(trainingData['review'], trainingData['sentiment'], train_size=0.8, test_size=0.2)\n",
        "\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    analyzer = CountVectorizer().build_analyzer()\n",
        "\n",
        "    # Do count vectorization\n",
        "    vectorizer = CountVectorizer(strip_accents='ascii',  binary=False)\n",
        "    vectors_train = vectorizer.fit_transform(X_train)\n",
        "    vectors_test = vectorizer.transform(X_test)\n",
        "\n",
        "    # Calculate term-frequencies and inverse document frequencies\n",
        "    tf_idf_vectorizer = TfidfVectorizer()\n",
        "    vectors_train_idf = tf_idf_vectorizer.fit_transform(X_train)\n",
        "    vectors_test_idf = tf_idf_vectorizer.transform(X_test)\n",
        "\n",
        "    # Normalize data\n",
        "    normalizer_train = Normalizer().fit(X=vectors_train)\n",
        "    vectors_train_normalized = normalizer_train.transform(vectors_train_idf)\n",
        "    vectors_test_normalized = normalizer_train.transform(vectors_test_idf)\n",
        "\n",
        "    # Run a GridSearchCV to determine optimal hyper-parameters (very long with many parameters)\n",
        "    nFolds = 5\n",
        "    parameters = {\n",
        "        'hidden_layer_sizes': [(10, 10, 10)],\n",
        "        'activation': ['relu'],\n",
        "        'solver': ['adam'],\n",
        "        'alpha': [0.0001],\n",
        "        'learning_rate': ['adaptive'],\n",
        "    }\n",
        "    clf = MLPClassifier(max_iter=100)\n",
        "    clf = GridSearchCV(clf, parameters, cv=nFolds, verbose=10, n_jobs=-1)\n",
        "    clf.fit(vectors_train_normalized, y_train)\n",
        "    print(\"Best score found during GridSearchVH\", clf.best_score_)\n",
        "    for param_name in sorted(parameters.keys()):\n",
        "        print(\"%s: %r\" % (param_name, clf.best_params_[param_name]))\n",
        "    y_pred = clf.predict(vectors_test_normalized)\n",
        "\n",
        "    print(\"Runtime =\", time.time() - startTime, \"seconds\")\n",
        "\n",
        "    # Save file if doing Kaggle submission, if not then print metrics\n",
        "    if trainOnEntireDataSet:\n",
        "        if inColab:\n",
        "            from google.colab import files\n",
        "\n",
        "            with open('submission.txt', 'w') as f:\n",
        "                f.write('id,sentiment\\n')\n",
        "                for x in range(len(y_pred)):\n",
        "                    f.write(str(x) + \",\" + y_pred[x] + \"\\n\")\n",
        "        else:\n",
        "            with open('submission.csv', 'w') as f:\n",
        "                f.write('id,sentiment\\n')\n",
        "                for x in range(len(y_pred)):\n",
        "                    f.write(str(x) + \",\" + y_pred[x] + \"\\n\")\n",
        "    else:\n",
        "        print(\"Report:\\n\", metrics.classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    9.5s\n",
            "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:   18.3s remaining:   12.2s\n",
            "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   23.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   23.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score found during GridSearchVH 0.8387499999999999\n",
            "activation: 'relu'\n",
            "alpha: 0.0001\n",
            "hidden_layer_sizes: (10, 10, 10)\n",
            "learning_rate: 'adaptive'\n",
            "solver: 'adam'\n",
            "Runtime = 31.929460287094116 seconds\n",
            "Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.88      0.77      0.82       297\n",
            "    positive       0.80      0.90      0.84       303\n",
            "\n",
            "    accuracy                           0.83       600\n",
            "   macro avg       0.84      0.83      0.83       600\n",
            "weighted avg       0.84      0.83      0.83       600\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}